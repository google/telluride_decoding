# Copyright 2019 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
r"""Regression Tests.

Test the decoding software with published data. This program reads and decodes
data from several different sources.  This code then builds a decoder, using
jackknifing to get a reliable estimate.

Results are computed for jackknifed experiments, each of which computes
correlation values for all the regularization_values using jackknifing over
max_test_count files

Example command line:

  regression --test_name 'telluride4_cca' --max_test_count 2 \
  --base_cache_dir /tmp/raw_data_storage/ --summary_base_dir /tmp/summary/ \
  --model_base_dir /tmp/models/ --logtostderr True \
  --tensorboard_dir '/tmp/tensorboard/' \
  --input2_pre_context=3 --input2_post_context=3 --post_context=3

"""
import collections
import os
import sys

from absl import app
from absl import flags
from absl import logging

import numpy as np
from telluride_decoding import brain_data
from telluride_decoding import csv_util
from telluride_decoding import decoding
from telluride_decoding import plot_util
import tensorflow as tf
# User should call tf.compat.v1.enable_v2_behavior()


regression_print = sys.stdout  # Feel free to redirect this output elsewhere.


# The run_number flag can be used here to specify the number of otherwise
# identical runs. Most of the other flags for setting model parameters are
# inherited from decoding.
flags.DEFINE_integer(
    'run_number', 1,
    'Run number, only used here so each run can get its own summary file.')
flags.DEFINE_integer(
    'max_test_count', -1,
    'Number of files to use when jackknifing.')
flags.DEFINE_string(
    'regularization_list', 'normal',
    'Regularization values for to be used when training model')
flags.DEFINE_enum(
    'test_name', 'telluride4_linear',
    ['telluride4_linear', 'telluride4_cca',
     'jens_memory_linear', 'jens_memory_cca',
     'iw0626_linear',
    ],
    ('Test to run in the form model_datasetname'))
flags.DEFINE_bool('cache', False,
                  'Should all data just be downloaded to a local cache?')
flags.DEFINE_string('test_file', None,
                    'Specify just one test file for jackknifing.'
                    '  This is usually set by the specific Regression model')

flags.DEFINE_string(
    'model_base_dir', os.path.join(os.environ.get('TMPDIR') or '/tmp', 'model'),
    'Base directory where the where the models (and checkpoints) are stored.'
    ' Trial-specific name is added to this directory.')
flags.DEFINE_string(
    'plot_base_dir', os.path.join(os.environ.get('TMPDIR') or '/tmp', 'plots'),
    'location of storing images generated by regression tests')
flags.DEFINE_string(
    'summary_base_dir',
    os.path.join(os.environ.get('TMPDIR') or '/tmp', 'summary'),
    'Directory where the final experiment results should be placed.'
    ' Trial-specific name is added to this directory.')
flags.DEFINE_string('results_csv_file', None,
                    'The CSV file to save the results.')


FLAGS = flags.FLAGS


def get_brain_data_object(my_flags):
  """Creates the BrainData object, pulling in all the FLAGS parameters.

  All of these flags are defined by the decoding file, and are necessary
  to tell the BrainData objects how to format the TFRecord data.

  Args:
    my_flags: Decoding flags for this data.

  Returns:
    The BrainData object that describes the training and testing data.
  """
  if not isinstance(my_flags, decoding.DecodingOptions):
    raise TypeError('get_brain_data_objects needs a DecodingOptions object, ' +
                    'not %s.' % type(my_flags))

  test_brain_data = brain_data.TFExampleData(
      my_flags.input_field,
      my_flags.output_field,
      my_flags.frame_rate,
      pre_context=my_flags.pre_context,
      post_context=my_flags.post_context,
      in2_fields=my_flags.input2_field,
      in2_pre_context=my_flags.input2_pre_context,
      in2_post_context=my_flags.input2_post_context,
      final_batch_size=my_flags.batch_size,
      data_dir=my_flags.tfexample_dir,
      train_file_pattern=my_flags.train_file_pattern,
      validate_file_pattern=my_flags.validate_file_pattern,
      test_file_pattern=my_flags.test_file_pattern,
      shuffle_buffer_size=my_flags.shuffle_buffer_size)
  return test_brain_data


def get_brain_model(test_dataset, my_flags):
  """Creates a brain model object for this regression test.

  Args:
    test_dataset: Some data we can use to set the input sizes.  Not otherwise
      read
    my_flags: A DecodingOptions object that we need for setting the model
      parameters.
  Returns:
    A BrainModel object, ready for training.
  """
  if not isinstance(my_flags, decoding.DecodingOptions):
    raise TypeError('Get_brain_model needs a DecodingOptions object, ' +
                    'not %s.' % type(my_flags))

  test_brain_model = decoding.create_brain_model(my_flags, test_dataset)
  return test_brain_model


def jackknife_one_model(test_brain_data,
                        test_brain_model,
                        model_dir,
                        my_flags,
                        max_test_count=-1,
                        test_name='telluride4',
                        trial_number=0,
                        summary_file=None,
                        test_file=None):
  """Function for running a jack knife experiment.

  This routine trains and tests a model on multiple independent training and
  test sets.  In the usual case, each possible file is used a test set, and
  all the other files are used for training. Just one value of all parameters,
  including regularization. Can be used for linear as well as CCA.
  Args:
    test_brain_data: BrainData object containing data
    test_brain_model: BrainModel object describing the ML model to run
    model_dir: Directory to save out the model
    my_flags: Decoding flags for this test.
    max_test_count: Max files to iterate over during jack knifing. -1 iterates
      through all files in the directory.
    test_name: data for which test is being run
    trial_number: Number printed out in the summary file to distinguish
      between trials.  (There are multiple trials per run, all trials go into
      the same summary file.)
    summary_file: Where to summarize the performance (either file pointer or
      file name)
    test_file: Just run this one test file (not the full jackknife)

  Returns:
    all_cor: List of float correlation values for the jack knifed run,
      one result per test file.
  """
  if not isinstance(my_flags, decoding.DecodingOptions):
    raise TypeError('Jackknife_one_model needs a DecodingOptions object, ' +
                    'not %s.' % type(my_flags))

  tf.keras.backend.clear_session()

  if test_file:
    all_test_files = [test_file]
  else:
    all_test_files = test_brain_data.all_files(max_test_count)

  all_cor = []
  for test_file in sorted(all_test_files):
    test_brain_data.set_file_patterns(my_flags.train_file_pattern, test_file,
                                      test_file)
    logging.info('Jackknife testing file %s with lambda=%g and metric %s',
                 test_file, my_flags.regularization_lambda,
                 my_flags.test_metric)
    if model_dir:
      my_flags.saved_model_dir = os.path.join(
          model_dir, 'test_%s' % (test_file,))
    else:
      my_flags.saved_model_dir = None

    # Note, this retrains from the last model state. This is probably ok for
    # linear and CCA, but perhaps not DNN. To be sure, recreate the model each
    # time.
    _, test_results = decoding.train_and_test(my_flags,
                                              test_brain_data, test_brain_model,
                                              epochs=my_flags.epoch_count)

    if my_flags.test_metric in test_results:
      all_cor.append(test_results[my_flags.test_metric])
    else:
      raise ValueError('Could not find metric %s in results %s.' %
                       (my_flags.test_metric, test_results))
    logging.info('Jackknife testing %s with lambda=%g got correlation %g',
                 test_file, my_flags.regularization_lambda,
                 test_results[my_flags.test_metric])
  log_entry = ('Jackknife test result test={}, regularization lambda={}, '
               'trial={}, mean correlation={}, std={}, '
               'test count={}\n'.format(
                   test_name, my_flags.regularization_lambda, trial_number,
                   np.mean(all_cor), np.std(all_cor), len(all_cor)))
  log_entry += 'Jackknife parameters:' + my_flags.experiment_parameters()
  log_entry += '\n'

  logging.info(log_entry)
  if summary_file:
    print('Writing summary data for one jackknife:', log_entry,
          file=regression_print)
    if isinstance(summary_file, str):
      with tf.io.gfile.GFile(summary_file, 'a') as fp:
        fp.write(log_entry)
        logging.info('Wrote jackknife summary results to %s', summary_file)
    else:
      summary_file.write(log_entry)
  return all_cor


def calculate_stats(run_results, axis=(1,)):
  """Calulates the mean and stdev across different holdout.

  files and different trials. Print out the output arrays.

  Args:
    run_results: Output of multiple runs results for a different
      regularization lambda values. It is a matrix of size (#lambda_values,
      num_test_files) matrix with one row of results for each regularization.
    axis: axis along which to compute stats
  Returns:
    run_mean: Mean correlation across runs
    run_std: Std of correlation across runs
  """
  run_mean = np.mean(run_results, axis=axis)
  run_std = np.std(run_results, axis=axis)
  return run_mean, run_std


def parse_regularization_values(mode_string):
  """Returns list of regularization values for different modes."""
  if isinstance(mode_string, float):
    return [mode_string,]
  if not isinstance(mode_string, str):
    raise TypeError('Parse_regularization_values needs a comma-separated' +
                    ' string, not a %s' % mode_string)
  mode_string = mode_string.lower()
  if mode_string == 'normal':
    return np.power(10, np.arange(-6.0, 0.5, 1))
  elif mode_string == 'test':
    return np.power(10, np.arange(-6.0, -5, 1))
  else:
    try:
      return np.array([float(tok) for tok in mode_string.split(',')],
                      dtype=np.float32)
    except:
      raise Exception('Could not parse regularization values: Want '
                      'comma separated list of floats, not %s' % mode_string)


class Regression(object):
  """Regression Test object, which runs a series of experiments over a dataset.

  We have subclasses of this base class for each model (regression and CCA as of
  now). Further down the chain, we have subclasses for different datasets. These
  are used to set the default flag parameters for the models to be run for each
  dataset.

    Args:
      dataset: RegressionData object containing details of the dataset on which
               to run the experiment
      frame_rate: Sampling rate of the dataset
  """

  def __init__(self, my_flags=None):
    if my_flags and not isinstance(my_flags, decoding.DecodingOptions):
      raise TypeError('Regression init needs a DecodingOptions object, ' +
                      'not %s.' % type(my_flags))
    self.my_flags = my_flags or decoding.DecodingOptions().set_flags()
    self.test_name = 'Regression Test Object'

  @property
  def model_type(self):
    return 'Undefined'

  def plot_results(self, regularization_list, run_mean, run_std):
    """Generates a plot of the results.

    Args:
      regularization_list: A list of regularizaiton values
      run_mean: the mean from each regularization value, over tests
      run_std: the standard deviation from each regularization value.
    """
    plot_util.plot_mean_std(
        self.test_name,
        regularization_list,
        run_mean,
        run_std,
        png_file_name=os.path.join(FLAGS.plot_base_dir,
                                   self.test_name + '_jack_knife.png'))

  def jackknife_over_regularizations(self,
                                     my_flags,
                                     regularization_list,
                                     test_file=None,
                                     summary_base_dir='/tmp',
                                     model_base_dir=None,
                                     max_test_count=-1):
    """Gets a dictionary of mean and std for the runs.

    Generate results across a range of regularization values.
    Call the jackknife routine to iterate over the test files.

    Note: both the summary_base_dir and model_base_dir give the basic directory
    name, and then create a subdirectory for each jackknife X regularization
    value.  These subdirectories have the name (called test_path_part):
      reglambda_{regularization_value}_test_{test_file_name}

    Args:
      my_flags: Decoding flags for this run.
      regularization_list: A list of regularization values (floats) to test.
      test_file: Which file to use for the test.  Either None for a complete
        jackknife, using all the available files for testing, or just one file.
      summary_base_dir: The base directory for the test summary results. Each
        jacknife result is then in a subdirectory named test_path_part described
        above.
      model_base_dir: The base directory for the final model. Each
        jacknife result is then in a subdirectory named test_path_part described
        above.
      max_test_count: How many test files across which to run cross validation
        (use a small number for tests) default -1 indicates all

    Returns:
      A dictionary with regularization as the key, and a tuple of mean and
        standard deviation as the value.
    """
    test_brain_data = get_brain_data_object(my_flags)
    all_files = test_brain_data.all_files(max_test_count)
    my_flags.train_file_pattern = my_flags.train_file_pattern or 'allbut'
    my_flags.validate_file_pattern = test_file or all_files[0]
    my_flags.test_file_pattern = test_file or all_files[0]
    print('Before creating the first dataset train, validate, and test:',
          my_flags.train_file_pattern, ',', my_flags.validate_file_pattern, ',',
          my_flags.test_file_pattern, file=regression_print)
    test_dataset = test_brain_data.create_dataset('test')

    logging.info('Regularization list is: %s', regularization_list)

    num_trials = len(test_brain_data.all_files(max_test_count))
    logging.info('num_trials is %s', num_trials)
    all_runs_results = np.zeros((len(regularization_list), num_trials),
                                dtype=float)

    logging.info('Creating brain_models with these parameters: \n  %s',
                 '\n  '.join(sorted(my_flags.experiment_parameters(None))))

    for reg_number, regularization_lambda in enumerate(regularization_list):
      test_path_part = 'reglambda_{}_test_{}'.format(regularization_lambda,
                                                     test_file)
      full_summary_dir = os.path.join(summary_base_dir, test_path_part)
      tf.io.gfile.makedirs(full_summary_dir)
      summary_file = os.path.join(full_summary_dir, 'results.txt')
      logging.info('Writing jackknife summary to %s', summary_file)

      my_flags.regularization_lambda = regularization_lambda
      test_brain_model = get_brain_model(test_dataset, my_flags)
      with tf.io.gfile.GFile(summary_file, 'w') as summary_fp:
        if model_base_dir:
          model_dir = os.path.join(model_base_dir, test_path_part)
        else:
          model_dir = None

        # Set flags again to update the model_dir
        self.preset_flags()
        logging.info('** Jackknife test number:%d, Regularization lambda: %f',
                     reg_number, regularization_lambda)
        correlation_output = jackknife_one_model(
            test_brain_data,
            test_brain_model,
            model_dir,
            my_flags,
            max_test_count=max_test_count,
            summary_file=summary_fp,
            test_file=test_file)
        all_runs_results[reg_number, :] = correlation_output
      logging.info('Wrote all trial summaries to %s.', summary_file)
    print(all_runs_results, file=regression_print)
    if FLAGS.results_csv_file:
      csv_util.write_results(FLAGS.results_csv_file, regularization_list,
                             all_runs_results)

    test_mean, test_std = calculate_stats(all_runs_results)
    mean_std_dict = collections.OrderedDict()
    for i in range(len(regularization_list)):
      mean_std_dict[regularization_list[i]] = (test_mean[i], test_std[i])
    return mean_std_dict

  def preset_flags(self):
    """Sets generic non  model specific flags.

    Every data source has its own parameters. For default, these parameters are
    purposedly set small so that the test is quick. For the real test
    comparison to the published results, one needs to reset the parameters
    on the command line.

    Returns:
      A string for the name of the model.
    """
    self.my_flags.batch_norm = True
    self.my_flags.batch_size = 100  # Small batches better for covariance comp.
    self.my_flags.data = 'tfrecords'
    self.my_flags.epoch_count = 1
    self.my_flags.input_field = 'eeg'
    self.my_flags.loss = 'mse'
    self.my_flags.output_field = 'intensity'
    self.my_flags.shuffle_buffer_size = 100
    if not self.my_flags.train_file_pattern:
      self.my_flags.train_file_pattern = 'allbut'
    return 'Generic'


class RegressionLinear(Regression):
  """Subclass for Linear Regression model."""

  def preset_flags(self):
    """Set flags for linear regression jack knife run."""
    super(RegressionLinear, self).preset_flags()
    self.my_flags.dnn_regressor = 'linear'
    if not self.my_flags.post_context:
      self.my_flags.post_context = 20
    self.my_flags.input2_pre_context = 0
    self.my_flags.input2_post_context = 0
    self.my_flags.input2_field = None
    self.my_flags.test_metric = 'pearson_correlation_first'
    self.my_flags.shuffle_buffer_size = 0  # No need for linear regression
    return 'linear'


class RegressionCCA(Regression):
  """Subclass for CCA model."""

  def preset_flags(self):
    """Sets flags for CCA jack knife run."""
    super(RegressionCCA, self).preset_flags()
    self.my_flags.dnn_regressor = 'cca'
    if not self.my_flags.post_context:
      self.my_flags.post_context = 21
    if not self.my_flags.input2_pre_context:
      self.my_flags.input2_pre_context = 15
    if not self.my_flags.input2_post_context:
      self.my_flags.input2_post_context = 15
    self.my_flags.input2_field = 'intensity'
    self.my_flags.output_field = 'eeg'
    self.my_flags.test_metric = 'cca_pearson_correlation_first'
    self.my_flags.shuffle_buffer_size = 0  # No need when training a CCA model
    logging.info('******* Fixing CCA dimensions to 5!!!')
    self.my_flags.cca_dimensions = 5
    return 'cca'


class JensMemoryCCA(RegressionCCA):
  """Class for Jens dataset CCA."""
  pass


class JensMemoryLinear(RegressionLinear):
  """Class for Jens dataset Linear Regression."""
  pass


class Telluride4Linear(RegressionLinear):

  def preset_flags(self):
    super(Telluride4Linear, self).preset_flags()
    self.my_flags.tfexample_dir = (self.my_flags.tfexample_dir or
                                   'test_data/'
                                   'tf_dir/telluride4_64Hz')


class Telluride4CCA(RegressionCCA):

  def preset_flags(self):
    super(Telluride4CCA, self).preset_flags()
    self.my_flags.tfexample_dir = (self.my_flags.tfexample_dir or
                                   'test_data/'
                                   'tf_dir/telluride4_64Hz')


class TFRecordsLinear(RegressionLinear):
  """Runs a linear experiment using data in TFRecord format."""

  def preset_flags(self):
    model_type = super(TFRecordsLinear, self).preset_flags()
    self.my_flags.output_field = 'loudness'
    self.my_flags.batch_size = 100  # Small batches are better for Linear.
    return model_type


class TFRecordsCCA(RegressionCCA):
  """Runs a CCA experiment using data in TFRecord format."""

  def preset_flags(self):
    model_type = super(TFRecordsCCA, self).preset_flags()
    self.my_flags.output_field = 'loudness'
    self.my_flags.batch_size = 100  # Small batches are better for CCA.
    return model_type


def select_regression_object(test_name, my_flags):
  """Selects the right regression object given the desired test name.

  Args:
    test_name: Which test to run.
    my_flags: Decoding flags for this test plan.

  Returns:
    The proper Regression() object, which tells us which flags to use by default
    and how to run the different jackknife tests.
  """
  if not isinstance(my_flags, decoding.DecodingOptions):
    raise TypeError('Select_regression_object needs a DecodingOptions object,' +
                    ' not %s.' % type(my_flags))
  test_name = test_name.lower()
  if test_name == 'telluride4_linear':
    test_obj = Telluride4Linear(my_flags)
  elif test_name == 'telluride4_cca':
    test_obj = Telluride4CCA(my_flags)
  elif test_name == 'jens_memory_cca':
    test_obj = JensMemoryCCA(my_flags)
  elif test_name == 'jens_memory_linear':
    test_obj = JensMemoryLinear(my_flags)
  else:
    raise TypeError('Illegal test name: {}'.format(FLAGS.test_name))
  return test_obj


def main(argv):
  if len(argv) > 1:
    logging.warning('WARNING: Non-flag arguments: %s', argv)

  my_flags = decoding.DecodingOptions().set_flags()

  test_obj = select_regression_object(FLAGS.test_name, my_flags)
  regularization_values = parse_regularization_values(
      FLAGS.regularization_list)
  if regularization_values is None:  # 0 is valid, so look for None
    raise ValueError('Did not get a list of regularization values from %s' %
                     FLAGS.regularization_values)

  test_obj.preset_flags()

  results = test_obj.jackknife_over_regularizations(
      my_flags, regularization_list=regularization_values,
      summary_base_dir=FLAGS.summary_base_dir,
      model_base_dir=FLAGS.model_base_dir,
      test_file=FLAGS.test_file)

  logging.info('Jackknife results are: %s', results)


if __name__ == '__main__':
  app.run(main)
